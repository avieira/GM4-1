\part{Programmation lin\'eaire, algotihme du simplexe}
\section{Introduction}
Un problème d'optimisation linéaire est un problème d'optimisation dans lequel le coût et les contraintes sont linéaires (ou plutôt affines).\\
Il s'agit de trouver les solutions $x\in\mathbb{R}^n$ du problème :
\begin{equation}\tag{$P_L$} \label{PL}
	\left\{ \begin{array}{c c c c} \inf_{x\in\mathbb{R}^n} & \multicolumn{3}{c}{\langle c,x\rangle} \\
						\text{s. c.}     & Ax &=& b \\
								& x &\geq& 0
	\end{array} \right.
\end{equation}

où $A$ est une matrice de raille $m\times n$, $b\in\mathbb{R}^m$, $c\in\mathbb{R}^n$.\\
$x\geq 0$ signifie que toutes les composantes de $x$ sont positives.\\
Ce problème est dit sous forme standard.

\bigskip
\underline{Remarque :} On a l'impression que \ref{PL} est un cas particulier du problème (sous forme canonique) : 
\begin{equation} \label{PC}
	\left\{ \begin{array}{c c c c} \inf_{x\in\mathbb{R}^n} & \multicolumn{3}{c}{\langle c,x\rangle} \\
						\text{s. c.}     & A'x &=& b' \\
								& Ax &\geq& b
	\end{array} \right.
\end{equation}

Mais un problème sous forme canonique peut toujours se ramener à un problème sous forme standard. En effet, la contrainte $A'x=b'$ est équivalent à $A'x\geq b'$ et $-A'x\geq -b'$. Donc \ref{PC} est équivalent à :
\begin{equation} \label{PC2} \inf_{x\in\mathbb{R}^n,\ Ax\geq b} \langle c,x\rangle\end{equation}

On introduit des variables d'écart $\lambda\in\mathbb{R}^n_+$ tel que $Ax=b+\lambda$. Donc \ref{PC2} se ramène à :
\[	\left\{ \begin{array}{c c c c} \inf_{x\in\mathbb{R}^n,\ \lambda\in\mathbb{R}^n_+} & \multicolumn{3}{c}{\langle c,x\rangle} \\
						\text{s. c.}     & Ax-\lambda &=& b \\
								& b &>& 0
	\end{array} \right.\]

On décompose $x$ sous la forme :
	\[x=x^+-x^-\]
où $x^+=\max (0,x) \geq 0$ et $x^-=-\min (0,x)\geq 0$ \\
\ref{PC2} revient donc à résoudre :
\[	\left\{ \begin{array}{c c c c} \inf_{x^+\in\mathbb{R}^n, x^-\in\mathbb{R}^n, \lambda\in\mathbb{R}^n}& \multicolumn{3}{c}{\langle c,x\rangle} \\
						\text{s. c.}     & Ax-\lambda &=& b \\
								& b &>& 0
	\end{array} \right.\]
qui est bien sous forme standard (mais avec plus de variables).

\bigskip
\underline{Remarque :} On peut supposer sans perte de généralité que toutes les lignes de $A$ sont linéairement indépendantes.\\
Si ce n'est pas le cas, soit certaines constraintes sont redondantes, soit les contraintes sont incompatibles, ie $rg(A)=m\leq n$

\Def{}{L'ensemble \[X_{ad}=\{x\in\mathbb{R}^n,\ Ax=b,\ x\geq 0\}\]
est appelé l'ensemble des solutions réalisables (ou admissibles).\\
On appelle sommet (ou point extremal) de $X_{ad}$ un point $x\in X_{ad}$ tel qu'il n'existe pas $\alpha\in]0,1[$ et $y,z\in X_{ad}$, $y\neq z$ tel que $x=\alpha y+(1-\alpha)z$.}

\section{Solutions de base d'un problème sous forme standard}
On note $A_1=\begin{pmatrix} a_{1,1} \\ \vdots \\ a_{m,1} \end{pmatrix}$, ..., $A_n=\begin{pmatrix} a_{1,n} \\ \vdots \\ a_{m,n} \end{pmatrix}$. \\
Comme $rg(A)=m$, on peut toujours trouver $m$ colonnes de $A$ linéairement indépendantes.\\
On note
	\[\Gamma=\{\gamma:\{1,...,m\}\to \{1,...,n\}, \text{ strictement croissante} \}\]
On définit :
	\[A_{\gamma}=(A_{\gamma(1)} \cdots A_{\gamma(m)})\]
et :
	\[\mathcal{B}=\{\gamma\in\Gamma,\ rg(A_{\gamma})=m\}\]

Pour $\gamma\in\Gamma$, on définit $\hat{\gamma}$ comme l'unique application strictement croissante de $\{1,...,n-m\}$ dans $\{1,...,n\}$ tel que :
	\[\gamma(\{1,...,m\})\cup\gamma(\{1,...,n-m\})=\{1,...,n\}\]

\Def{}{Pour $\gamma\in\mathcal{B}$, la matrice $A_{\gamma}$ est appelée base associée à $\gamma$.\\
Les composantes $(x_{\gamma(1)},...,x_{\gamma(m)})$ sont appelées les composantes de base, et les composantes $(x_{\hat{\gamma}(1)},...,x_{\hat{\gamma}(n-m)})$ sont appelées les composantes hors base.}

Pour $\gamma\in\mathcal{B}$, on note
	\[x_{\mathcal{B}}=(x_{\gamma(1)},...,x_{\gamma(m)})\]
	\[x_{N}=(x_{\hat{\gamma}(1)},...,x_{\hat{\gamma}(n-m)})\]
	\[B=A_{\gamma}\]
	\[N=A_{\hat{\gamma}}\]
\[Ax=Bx_{\mathcal{B}}+Nx_N\]

Comme $rg(B)=rg(A_{\gamma})=m$, B est inversible. Donc les contraintes $Ax=b$ peuvent se réécrire :
	\[Bx_{\mathcal{B}}=b-Nx_N\]
	\[\Rightarrow x_B=B^{-1}(b-Nx_N)\]

\Def{}{Soit $\gamma\in\mathcal{B}$, on appelle solution de base du système $Ax=b$ associé à la base $\gamma$ la solution $x^*$ définie par :
	\[\begin{array}{c c c}
	x_{\mathcal{B}}^*&=&B^{-1}b\\
	x_N^*&=&0
	\end{array}\]}


\Def{}{Une solution de base réalisable est une solution de base telle que $x_{\mathcal{B}}^*\geq0$ ($\rightarrow x^*\in X_{ad}$).\\
Dans ce cas, $\gamma$ est appelée base réalisable. On note $\mathcal{R}$ l'ensemble des bases réallisables.\\
Enfin on dit que $x^*$ est non dégénéré si $x_{\mathcal{B}}^*>0$ (ie $B^{-1}b>0$).}

\Lem{}{Les sommets de $X_{ad}$ sont exactement les solutions de base réalisable.}

\begin{dem}
Soit $x^*$ une solution de base réalisable associée à la base $\gamma\in\mathcal{R}$.\\
Par l'absurde, on va supposer que $x^*$ n'est pas un sommet de $X_{ad}$.
	\[\forall \theta\in]0,1[, \exists y,z\in X_{ad}, y\neq z; x^*=\theta y+(1-\theta)z\]
$\forall i\in\{0,...,n-m\}$, on a :
	\[x_{\hat{\gamma}(i)}=0=\underbrace{\theta}_{>0} \underbrace{y_{\hat{\gamma}(i)}}_{\geq 0} + \underbrace{(1-\theta)}_{>0}\underbrace{z_{\hat{\gamma}(i)}}_{\geq 0}\]

	\[\Rightarrow y_{\hat{\gamma}(i)}=z_{\hat{\gamma}(i)}=0\]
	\[\Rightarrow y_N=z_N=0\]

Or, $y_B=B^{-1}(b-Ny_N)=B^{-1}b=z_B=x^*_B$. Donc $y=z=x^*$, ce qui est absurde.\\
Donc $x^*$ est un sommet de $X_{ad}$.

\bigskip
Réciproquement, on suppose que $x^*$ est un sommet de $X_{ad}$. On note $k$ le nombre de composantes non nulles de $x^*$. 
\[\exists \gamma_1 : \{1,...,k\}\to\{1,...,n\} \text{ strictement croissante telle que } x^*_{\gamma_1(i)}>0,\ \forall i=1,...,k\]
On veut montrer que les vecteurs $A_{\gamma_1(1)},...,A_{\gamma_1(k)}$ sont libres. Si c'est le cas, on pourra compléter cette famille de vecteurs afin d'obtenir une base.\\
($\Rightarrow$ On définit $\gamma\in\Gamma$ telle que :\begin{itemize}
\item $\gamma_1(\{1,...,k\})\subset\gamma(\{1,...,m\})$
\item $\gamma\in\mathcal{B}$)
\end{itemize}

Par l'absurde, on suppose que $A_{\gamma_1(1)},...,A_{\gamma_1(k)}$ sont liés. Alors $\exists y\in\mathbb{R}^n$, $y\neq 0$, 
\[\sum_{i=1}^k y_{\gamma_1(i)}A_{\gamma_1(i)}=0\]
et \[y_{\hat{\gamma}_1(i)}=0\]
$\forall \varepsilon>0$, on a \[A(x+\varepsilon y)=Ax+\varepsilon \underbrace{Ay}_{=0}=b\]
\[A(x-\varepsilon y)=Ax-\varepsilon Ay = b\]
On a $x^*_{\gamma_1(i)}\pm \varepsilon y_{\gamma_1(i)}>0$, $\forall i=1,...,k$ pour $\varepsilon$ assez petit.\\
De plus, $x^*_{\hat{\gamma}_1(i)}\pm \varepsilon y_{\hat{\gamma}_1(i)}=x^*_{\hat{\gamma}_1(i)}=0$
	\[\Rightarrow x^*\pm\varepsilon y\in X_{ad},\ \forall0\leq \varepsilon \leq \varepsilon_0\]
Or, $x^*=\frac{1}{2}(x^*+\varepsilon y) + \frac{1}{2}(x^*-\varepsilon y)$. Donc $x^*$ n'est pas un sommet de $X_{ad}$. Contradiction.
\end{dem}

\Lem{}{S'il existe une solution optimale de $(P_L)$ alors il existe une solution optimale de base réalisable.}

\begin{dem}
Soit $x\in X_{ad}$ une solution optimale. On note $k$ le nombre de composantes non nulles de $x$. Soit $\gamma_1:\{1,...,k\}\to \{1,...,n\}$ strictement croissante telle que $x_{\gamma_1(i)}>0$ $\forall i=1,...,k$.\\
Si la famille $A_{\gamma_1(1)},...,A_{\gamma_1(k)}$ est libre alors $x$ est une solution optimale de base réalisable.\\
Si la famille $A_{\gamma_1(1)},...,A_{\gamma_1(k)}$ est liée, alors $\exists y\in\mathbb{R}^n$ tel que :
	\[\sum_{i=1}^k y_{\gamma_1(i)} A_{\gamma_1(i)}=0\]
	\[y_{\hat{\gamma}_1(i)}=0\ \forall i 1\leq i\leq n-k\]
$x+\varepsilon y\in X_{ad}$ pour $\varepsilon$ assez petit (cf démo précédente). \\
Comme $c$ est un point de minimum, on a :
	\[\langle c,x\rangle \leq \langle c, x\pm \varepsilon y\rangle\]
	\[\Rightarrow \pm\langle c, y\rangle \geq 0\]
	\[\langle c,y\rangle=0\]
On pose $\varepsilon_1=\sup\{\varepsilon>0;\ x\pm\varepsilon y\in X_{ad}\}$. 
	\[\exists i\in \{1,...,k\}, 0<\varepsilon<\varepsilon_1 \text{ tels que } (x+\varepsilon y)_{\gamma_1(i)}=0 \text{ ou } (x-\varepsilon y)_{\gamma_1(i)}=0\]
On pose $z=x+\varepsilon y$ ou $z=x-\varepsilon y$. tel que $z_{\gamma_1(i)}=0$. z est donc une solution optimale 
\[(\langle c,z\rangle=\langle c,x\pm\varepsilon y\rangle = \langle c,x\rangle)\]
qui a au plus $k-1$ composantes non nulles.\\
On peut refaire la preuve en remplaçant $x$ par $z$ (et en diminuant $k$) jusqu'à obtenir une famille libre.
\end{dem}

\underline{Remarque :} D'après les deux propositions précédentes, en prenant $c=0$ (toutes les solutions admissibles sont optimales), si $X_{ad}=\emptyset$, alors $X_{ad}$ possède au moins 1 sommet (ceci n'est pas le cas pour un polyèdre quelconque).

\section{Algorithme du simplexe}
\underline{Idée :} Parcourir les sommets du polyèdre en diminuant le coût en passant d'un sommet au suivant.

\Lem{}{Soit $\gamma\in\mathcal{R}$ et $x^*$ la solution de base réalisable associée.\\
Pour $x\in X_{ad}$ réalisable, on a :
	\[x_B=x^*_B-B^{-1}Nx_N\]
	\[c^Tx=c^Tx^*+d^Tx\]
où $d^T_N=c_N^T-c_B^TB^{-1}N$ et $d^T_B=0$.}

\begin{dem}
\begin{eqnarray*}
	Ax&=&Bx_B+Nx_N=b\\
\rightarrow x_B&=&B^{-1}b-B^{-1}Nx_N
\end{eqnarray*}
Or, $x^*_B=B^{-1}b$, donc \[x_B=x^*_B-B^{-1}Nx_N\]

\begin{eqnarray*}
	c^Tx&=&c_B^Tx_B+c^T_Nx_n\\
	&=&c^T_B(x^*_B-B^{-1}Nx_N)+c^T_N x_N\\
	&=&c^T_Bx_B^*+(c^T_N-c^T_BB^{-1}N)x_N\\
	&=&c^Tx^*+d^Tx
\end{eqnarray*}
\end{dem}

\Def{}{On dit que le vecteur $d$ est le veteur des prix marginaux associé à la base $\gamma$.}

\Propo{}{Soit $\gamma$ une base réalisable et $x^*$ la solution de base associée.\\
Si toutes les composantes de $d$ sont positives, alors $x^*$ est une solution otpimale.\\
Si de plus, $x^*$ est non dégénérée ($B^{-1}b>0$) et est une solution optimale, alors les composantes de $d$ sont positives.}

\begin{dem}
Soit $x$ une solution réalisable. alors 
\[c^Tx=c^Tx^*+\underbrace{d^T}_{\geq 0}\underbrace{x}_{\geq 0}\geq c^Tx^*\]
$x^*$ est donc une solution optimale.

\bigskip
Réciproquement, on suppose que $x^*$ est une solution optimale non dégénérée.\\
Raisonnons par l'absurde : supposons que la ième composante de $d$ est strictement négative.\\
Pour $\varepsilon>0$, on définit :
\begin{eqnarray*}
	x_N(\varepsilon)&=&\varepsilon a_i\\
	x_B(\varepsilon)&=&B^{-1}b-B^{-1}Nx_N(\varepsilon)
\end{eqnarray*}

On a :
\begin{eqnarray*}
	Ax(\varepsilon)&=&Bx_B(\varepsilon)+Nx_N(\varepsilon)\\
			&=&b
\end{eqnarray*}
Comme $\varepsilon>0$ on a $x_N(\varepsilon)>0$.\\
Comme $B^{-1}b>0$, pour $\varepsilon$ assez petit, on a $x_B(\varepsilon)\geq 0$.\\
\[\Rightarrow x(\varepsilon)>0\]
$\Rightarrow$ $x(\varepsilon)$ est une solution réalisable.

\bigskip
On a également :
\begin{eqnarray*}
	c^Tx(\varepsilon)&=&c^Tx^*+d^Tx(\varepsilon)\\
		&=&c^Tx^*+d^T_Nx_N(\varepsilon)\\
		&=&c^Tx^*+\varepsilon \underbrace{d^T_{\hat{\gamma}(i)}}_{<0}\\
		&<&c^Tx^*
\end{eqnarray*}
$\Rightarrow$ $x^*$ n'est pas otimal. Absurde.
\end{dem}

\subsection{Privot à partir d'une base réalisable : critère de Dantzig}
Étant donné un choix de base $\gamma\in B$, tel que $x^*$ soit une solution de base réalisable mais pas optimale, le but est de trouver une nouvelle base $\delta\in B$ tel que la solution de base $y^*$ associée à $\delta$ soit réalisable et telle que 
\[c^Ty^*\leq c^Tx^*\]
Cette méthode opère au moyen d'un pivot dans le sens où $\gamma(\{1,...,m\})$ et $\delta(\{1,...,m\})$ ne diffèrent que par 1 seul élément.\\
En pratique, on note 
	\[E_{\gamma}=\{j\in\{1,...,n-m\};\ d_{\hat{\gamma}(j)}<0\}\]
Si la solution de base $x^*$ associée à $\gamma$ n'est pas optimale, alors $E_{\gamma}\neq \emptyset$.\\
Soit $j^*\in E_{\gamma}$ (plusieurs choix possibles, on verra ça plus tard). On définit l'ensemble 
	\[S_{\gamma,j^*}=\left\{i\in\{1,...,m\};\ (B^{-1}N)_{i,j^*}>0 \right\}\]

\Propo{}{Si $S_{\gamma,j^*}=\emptyset$, alors le problème $(P_L)$ n'admet pas de solution car la fonction coût n'est pas bornée inférieurement sur $X_{ad}$.}

\begin{dem}
	Soit $t>0$. On considère le vecteur $x$ définit par :
\begin{eqnarray*}
	x_{\hat{\gamma}(j)}&=&0 \text{ si } j\in\{1,...,n-m\}\textbackslash \{j^*\}\\
	x_{\hat{\gamma}(j^*)}&=&t\\
	x_{\gamma(i)}&=&(B^{-1}b)_i-t(B^{-1}N)_{ij^*} \text{ si } i\in\{1,...,m\}
\end{eqnarray*}

On a $Ax=b$ par construction.\\
De plus, $x_N\geq 0$ et $x_B\geq 0$ car $(B^{-1}N)_{ij^*}\leq 0$ (car $S_{\gamma,j^*}=\emptyset$).\\
$\Rightarrow$ $x$ est une solution admissible.

\bigskip
De plus :
\begin{eqnarray*}
	c^Tx&=&c^Tx^*+d^Tx\\
		&=&c^Tx^*+d_N^Tx_N\\
		&=&c^Tx^*+\underbrace{d_{\hat{\gamma}(j^*)}}_{<0}t
\end{eqnarray*}

Quand $t\to +\infty$, on a $c^Tx\to -\infty$, avec $x\in X_{ad}$. La fonction coût n'est donc pas bornée inférieurement.
\end{dem}

On suppose à partir de maintenant que la fonction coût est bornée inférieurement.
	\[\Rightarrow S_{\gamma,j^*}\neq \emptyset\]
On choisit $i^*\in S_{\gamma,j^*}$.

\Lem{}{Soit $\delta\in\Gamma$ l'unique application telle que 
	\[\delta(\{1,...,m\})=\gamma(\{1,...,m\})\backslash\gamma(i^*)\cup\hat{\gamma}(j^*)\]
Alors $\delta\in B$.}

\begin{dem}
Montrons que :
	\[A_{\hat{\gamma}(j^*)}=\sum_{i=1}^m (B^{-1}N)_{ij^*}A_{\gamma(i)}\]
\begin{eqnarray*}
	\sum_{i=1}^m (B^{-1}N)_{ij^*}A_{\gamma(i)}&=&\sum_{i=1}^m \sum_{j=1}^m B^{-1}_{ij}N_{jj^*}A_{\gamma(i)}\\
			&=&\sum_{j=1}^m N_{jj^*} \sum_{i=1}^m B^{-1}_{ij}\begin{pmatrix} B_{1i}\\ \vdots \\ B_{mi} \end{pmatrix} \\
			&=&\sum_{j=1}^m N_{jj^*} \begin{pmatrix} \sum_{i=1}^m B_{1i}B^{-1}_{ij} \\ \vdots \\ \sum_{i=1}^m B_{mi}B^{-1}_{ij} \end{pmatrix} \\
			&=&\sum_{j=1}^m N_{jj^*} \begin{pmatrix} (BB^{-1})_{1j} \\ \vdots \\ (BB^{-1})_{mj} \end{pmatrix}\\
			&=&\sum_{j=1}^m N_{jj^*}e_j \\
			&=&A_{\hat{\gamma}(j^*)}
\end{eqnarray*}
Comme $\{A_{\gamma(i)}\}_{i=1}^m$ forme une base et que $(B^{-1}N)_{i^*j^*}>0$, on ne peut pas exprimer $A_{\hat{\gamma}(j^*)}$ comme une combinaison linéaire de $A_{\gamma(i)}$ pour $i=1,...,n$ avec $i\neq i^*$. \\
Donc $((A_{\gamma(i)})_{i=1,...,m,\ i\neq i^*},A_{\hat{\gamma}(j^*)})$ forme une base.
	\[\Rightarrow \delta \in B\]
\end{dem}

\Lem{Critère de Dantzig}{On choisit $i^*$ tel que $\frac{x_{\gamma(i^*)}}{(B^{-1}N)_{i^*j^*}}=t_{j^*}=\min_{i\in S_{\gamma,j^*}}\frac{x_{\gamma(i)}}{(B^{-1}N)_{ij^*}}$. Alors $\delta$ défini dans le lemme précédent est une base réalisable. De plus, si $y^*$ désigne la solution de base réalisable associée à $\delta$ alors
	\[c^Ty^*\leq c^Tx^*\]
l'inégalité étant stricte si $t_{j^*}>0$.}

\begin{dem}
On définit $y\in\mathbb{R}^n$ par :
	\[y_{\hat{\gamma}(i)}=0\ \forall i\in\{1,...,n-m\}\backslash\{j^*\}\]
	\[y_{\hat{\gamma}(j^*)}=t_{j^*}\]
	\[y_{\gamma(i)}=x^*_{\gamma(i)}=t_{j^*}(B^{-1}N)_{i,j^*}\ \forall i\in\{1,...,m\}\]
où $x^*$ est la solution de base réalisable associée à $\gamma$.
\begin{eqnarray*}
	Ay&=&By_B+Ny_N\\
	  &=&B(x^*_B-t_{j^*}(B^{-1}N)_{\bullet,j^*})+Ny_N\\
	  &=&Bx_B^*\\
	  &=&b
\end{eqnarray*}
De plus, $y\geq 0$ (par définition de $t_{j^*}$).\\
Donc $y$ est une solution réalisable.
	\[y_{\gamma(i^*)}=\hat{x}_{\gamma(i^*)}-t_{j^*}(B^{-1}N)_{i^*j^*}=0\]
et $y_{\hat{\gamma}(i)}=0$ $\forall i\in\{1,...,n-m\}\backslash\{j^*\}$. Donc $y_{\hat{\delta}(j)}=0$, $\forall j\in\{1,...,n-m\}$.\\
Donc $y$ est la solution de base réalisable associée à la base $\delta$. Donc $\delta$ est une base réalisable.
\begin{eqnarray*}
	c^Ty^*&=&c^Ty\\
		&=&c^Tx^*+d^Ty\\
		&=&c^Tx^*+d_{\hat{\gamma}(j^*)}t_{j^*}
\end{eqnarray*}
car :
\begin{eqnarray*}
	d^Ty&=&d^T_Ny_N+\underbrace{d^T_B}_{=0}y_B\\
	    &=&d^T_N \begin{pmatrix} 0 \\ \vdots \\ t_{j^*} \\ \vdots \end{pmatrix} \\
	    &=& d_{\hat{\gamma}(j^*)}t_{j^*}
\end{eqnarray*}

\[\Rightarrow c^Ty^*=c^Tx^*+\underbrace{d_{\hat{\gamma}(j^*)}}_{<0}\underbrace{t_{j^*}}_{\geq 0}\leq c^Tx^*\]
\end{dem}

\Def{Critère naturel}{On appelle variable entrante selon le critère naturel la variable $x_{\hat{\gamma}(j^*)}$ tel que :
	\[d_{\hat{\gamma}(j^*)}=\min_{j\in\{1,...,n-m\}}d_{\hat{\gamma}(j)}\]
et $j^*=\min\{j,\ d_{\hat{\gamma}(j)}=d_{\hat{\gamma}(j^*)}\}$.
On appelle variable sortante selon le critère naturel la variable $x_{\gamma(i^*)}$ tel que :
	\[i^*=\min\left\{i\in S_{\gamma,j^*}, \frac{x_{\gamma(i)}}{(B^{-1}N)_{i,j^*}}\right\}=t_{j^*}\]}

\Def{Critère de Bland}{On choisit la variable entrante selon le critère de Bland pour que $x_{\hat{\gamma}(j^*)}$ soit telle que :
	\[j^*=\min \{j, j\in E_{\gamma}\}\]
et la variable sortante $x_{\gamma(i^*)}$ telle que :
	\[i^*=\min\left\{i\in S_{\gamma,j^*}, \frac{x_{\gamma(i)}}{(B^{-1}N)_{i,j^*}}\right\}=t_{j^*}\]}

\subsection{Détermination d'une première base réalisable}
On considère le problème :
\begin{equation} \tag{*} \label{1reBase} \left\{ \begin{array}{c c c c} \inf_{x\in\mathbb{R}^q} & \multicolumn{3}{c}{c^Tx} \\
						\text{s. c.}     & Ax &\leq& b \\
								& x &\geq& 0
	\end{array} \right.\end{equation}

\[A=(a_{ij})_{1\leq i\leq n,\ 1\leq j\leq q},\ b\in\mathbb{R}^m,\ c\in\mathbb{R}^q \]

\Def{}{Le problème (\ref{1reBase}) est un problème de première espèce si toutes les composantes de $b$ sont positives.\\
Dans le cas contraire, ou si le problème n'est pas sous forme canonique, on dit qu'il s'agit d'un problème de deuxième espèce.}

On considère pour le moment un problème de première espèce ($b\geq 0$).

\[(*)\Leftrightarrow 	\left\{ \begin{array}{c c c c} \inf_{x\in\mathbb{R}^n} & \multicolumn{3}{c}{\bar{c}^T\bar{x}} \\
						\text{s. c.}     & \bar{A}\bar{x} &=& \bar{b} \\
								& \bar{x} &\geq& 0
	\end{array} \right.\]
où $\bar{x}=(x_1,...,x_{q+m})^T$, \[\bar{A}=\begin{pmatrix} a_{11} & \cdots & a_{1q} & 1 & \cdots & 0 \\
							    \vdots & \ddots & \vdots & \vdots & \ddots & \vdots \\
							    a_{m1} & \cdots & a_{nq} & 0 & \cdots & 1 \end{pmatrix}\]
$\bar{c}=(c_1,...,c_q,0,...,0)^T$\\
$\bar{b}=b$

\Lem{}{On suppose que toutes les composantes de $b$ sont positives. Alors le problème (\ref{1reBase}) possède comme base réalisable celle obtenue en ne gardant en base que les $m$ variables d'écart.}

\begin{dem}
	On définit $\gamma$ par :
	\[\gamma(\{1,...,m\})=\{q+1,...,q+m\}\]
	Alors la matrice B correspondante est la matrice identité qui est inversible.\\
	De plus, $x^*=B^{-1}b=b>0$ \\
	Donc $x^*$ est une solution de base réalisable.
\end{dem}

On doit maintenant traiter des problèmes de deuxième espèce.\\
On commence par le cas d'un problème sous forme standard.
\begin{equation}\tag{1} \label{1erBas2}
	\left\{ \begin{array}{c c c c} \inf_{x\in\mathbb{R}^n} & \multicolumn{3}{c}{\langle c,x\rangle} \\
						\text{s. c.}     & Ax &=& b \\
								& x &\geq& 0
	\end{array} \right.
\end{equation}

On peut supposer que toutes les composantes de $b$ sont positives (sinon, il suffit de multiplier la ligne correspondante de A par -1).\\
On introduit alors des variables "fictives" $y_1,...,y_m$ et on considère le problème :
\begin{equation}\tag{2} \label{1erBas3}
	\left\{ \begin{array}{c c c c} \inf_{x\in\mathbb{R}^n} & \multicolumn{3}{c}{y_1+...+y_m} \\
						\text{s. c.}     & Ax +y&=& b \\
								& x,y &\geq& 0
	\end{array} \right.
\end{equation}


Si le problème (\ref{1erBas2}) admet une solution réalisable, alors le minimum dans (\ref{1erBas3}) est nul et inversement.\\
L'avantage est que le problème (\ref{1erBas3}) admet une solution de base réalisable triviale (x=0,y=b).

\bigskip
On peut alors appliquer l'algorithme du simplexe pour trouver une solution de base réalisable optimale.\\
Si le minimum obtenu est 0 ($y_1=...=y_m=0$), alors le $x$ correspondant est une solution de base réalisable pour (\ref{1erBas2}). 

\bigskip
On considère maintenant un problème sous forme canonique avec $b$ qui n'est pas positif.
\[\left\{ \begin{array}{c c c c} \inf_{x\in\mathbb{R}^n} & \multicolumn{3}{c}{\langle c,x\rangle} \\
						\text{s. c.}     & Ax &\leq& b \\
								& x &\geq& 0
	\end{array} \right.\]

Ce problème est équivalent à :
\begin{equation} \tag{1} \left\{ \begin{array}{c c c c} \inf_{x\in\mathbb{R}^{q+m}} & \multicolumn{3}{c}{\bar{c}^T\bar{x}} \\
						\text{s. c.}     & \bar{A}\bar{x} &=& \bar{b} \\
								& \bar{x} &\geq& 0
	\end{array} \right.\end{equation}

où $\bar{x}=(x_1,...,x_{q+m})^T$, \[\bar{A}=\begin{pmatrix} a_{11} & \cdots & a_{1q} & 1 & \cdots & 0 \\
							    \vdots & \ddots & \vdots & \vdots & \ddots & \vdots \\
							    a_{m1} & \cdots & a_{nq} & 0 & \cdots & 1 \end{pmatrix}\]
$\bar{c}=(c_1,...,c_q,0,...,0)^T$\\
$\bar{b}=b$

On considère le problème d'initialisation :
\begin{equation} \tag{2} \left\{ \begin{array}{c c c c} \inf_{x\in\mathbb{R}^{q+m+1}} & \multicolumn{3}{c}{x_{q+m+1}} \\
						\text{s. c.}     & \bar{\bar{A}}x &=& b \\
								& x &\geq& 0
	\end{array} \right.\end{equation}

\[\bar{\bar{A}}=\begin{pmatrix} a_{11} & \cdots & a_{1q} & 1 & \cdots & 0 & -1\\
				\vdots & \ddots & \vdots & \vdots & \ddots & \vdots & \vdots\\
				a_{m1} & \cdots & a_{nq} & 0 & \cdots & 1 & -1\end{pmatrix}\]

Le problème (1) admet une solution réalisable si et seulement si le minimum du problème (2) est 0.\\
Pour réaliser le problème (2) par la méthode du simplexe, il suffit de trouver une première base réalisable pour (2).

\bigskip
Pour ce faire, on définit $i_0\in\{1,...,m\}$ tel que \[b_{i_0}=\min_{i\in\{1,...,m\}} b_i<0\]
Pour les variables de base, on choisit : \[\{x_{q+1},...,x_{q+m+1}\}\textbackslash\{x_{q+i_0}\}\]
Le système $\bar{\bar{A}}x=b$ se traduit par :
	\[x_{q+i}-x_{q+m+1}=b_i,\ \forall i\neq i_0\]
	\[x_{q+m+1}=-b_{i_0}>0\]
$\forall i\neq i_0$, $x_{q+i}=b_i-b_{i_0}\geq 0$. On a donc bien une base réalisable.

\section{Dualité en programmation linéaire}
On condidère un problème sous forme canonique :
\begin{equation}\tag{$P$} \label{Dual}
	\left\{ \begin{array}{c c c c} \inf_{x\in\mathbb{R}^n} & \multicolumn{3}{c}{\langle c,x\rangle} \\
						\text{s. c.}     & Ax &\leq& b \\
								& x &\geq& 0
	\end{array} \right.
\end{equation}

On suppose que (\ref{Dual}) possède au moins une solution optimale $x^*$. \\
La méthode du simplexe permet à chaque itération d'améliorer une borne supérieure sur le coût $c^Tx^*$. Peut-on obtenir une borne inférieure ? Pour cela, on cherche un esolution du problème dual.\\
Pour ce faire, on part des $m$ contraintes d'inégalités :
	\[\sum_{j=1}^q a_{ij}x_j\leq b_i,\ i=1,...,m\]
On prend $y_i>0$ : 
\begin{eqnarray*}
	\sum_{i=1}^m y_i \sum_{j=1}^q a_{ij}x_j&\leq& \sum_{i=1}^m y_ib_i\\
	\sum_{j=1}^q \left( \sum_{i=1}^m -y_ia_{ij}\right) x_j&\geq& -\sum_{i=1}^m y_ib_i
\end{eqnarray*}

Ainsi, \[-\sum_{i=1}^m y_ia_{ij} \leq c_j \Rightarrow \sum_{j=1}^q c_jx_j \geq -\sum_{i=1}^m y_ib_i\]

Donc $c^Tx^*\geq -b^Ty$, $\forall y\geq 0,\ y\in\mathbb{R}^m$ vérifiant \[-\sum_{i=1}^m y_ia_{ij} \leq c_j,\ \forall j\in\{1,...,q\}\Leftrightarrow -A^Ty\leq c\]

\Def{}{Le problème dual de (\ref{Dual}) est le problème suivant :
\begin{equation}\tag{$D$} \label{Dual2}
	\left\{ \begin{array}{c c c c} \max_{y\in\mathbb{R}^m} & \multicolumn{3}{c}{-\langle b,y\rangle} \\
						\text{s. c.}     & -A^Ty &\leq& c \\
								& y &\geq& 0
	\end{array} \right.
\end{equation}}

\Theo{}{Si $x$ est une solution réalisable de (\ref{Dual}), et si $y$ est une solution réalisable de (\ref{Dual2}), alors \[x^Tc\geq -b^Ty\]
En particulier, si $c^Tx=-b^Ty$, alors $x$ est une solution optimale de (\ref{Dual}) et $y$ est une solution optimale de (\ref{Dual2}).}

\Coro{}{Si la fonction coût n'est pas minorée sur l'ensemble admissible, alors le problème dual n'admet pas de solution réalisable.\\
Inversement, si le problème dual n'est pas majoré sur son ensemble admissible, alors le problème primal n'admet pas de solution réalisable.}

\underline{Remarque :} La réciproque est fausse en général.

\Theo{}{Le problème primal (\ref{Dual}) possède une solution optimale $x^*$ si et seulement si le problème dual possède une solution optimale $y^*$. Dans ce cas, on a $c^Tx^*=-b^Ty^*$.}

\begin{dem}
Il suffit de montrer que si $x^*=(x_1^*,...,x_q^*)$ est une solution optimale de (\ref{Dual}), alors il existe une solution réalisable $y^*=(y_1^*,...,y_n^*)$ de (\ref{Dual2}) tel que : \[c^Tx^*=-b^Ty^*\]
On part du problème primal. On y introduit les variables d'écart :
	\[x_{q+i}=b_i-\sum_{j=1}^q a_{ij}x_j\]
Le problème sous forme standrard est :
 \begin{equation}
	\left\{ \begin{array}{c c c c} \inf_{x\in\mathbb{R}^{q+m}} & \multicolumn{3}{c}{\langle \bar{c},x\rangle} \\
						\text{s. c.}     & \bar{A}x &=& b \\
								& x &\geq& 0
	\end{array} \right.
\end{equation}
avec $\bar{c}=(c_1,...,c_q,0,...,0)^T$ et :
\[\bar{A}=\begin{pmatrix} a_{11} & \cdots & a_{1q} & 1 & \cdots & 0 \\
			    \vdots & \ddots & \vdots & \vdots & \ddots & \vdots \\
			    a_{m1} & \cdots & a_{nq} & 0 & \cdots & 1 \end{pmatrix}\]
On note $\gamma$ la base réalisable associée à :
	\[\bar{x}^*=\left(x_1^*,..,x^*_q,b_1-\sum_{j=1}^q a_{1j}x_j^*,..., b_m-\sum_{j=1}^q a_{mj}x_j^*\right)\]

On note $d$ le vecteur des prix marginaux. $(d_B=0, d_N^T=c_N^T-c_B^TB^{-1}N)$\\
Soit $\bar{x}\in\mathbb{R}^{q+m}$ tel que $\bar{A}\bar{x}=b$. On a : 
\begin{eqnarray*}
	c^T\bar{x}&=&c^Tx^* + d^T\bar{x} \\
		&=& c^Tx^* + \sum_{i=1}^q d_i\bar{x}_i + \sum_{i=1}^m d_{q+i}\bar{x}_{q+i}
\end{eqnarray*}
Or, $\bar{x}_{q+i}=b_i-\sum_{j=1}^q a_{ij}\bar{x}_j$
\begin{eqnarray*}
	\Rightarrow c^Tx&=&c^Tx^* + \sum_{i=1}^q d_i\bar{x}_i + \sum_{i=1}^m d_{q+i}(b_i-\sum_{j=1}^q a_{ij}\bar{x}_j) \\
			&=&c^Tx^* + \sum_{i=1}^m d_{q+i}b_i + \sum_{j=1}^q \left(d_j-\sum_{i=1}^m a_{ij}d_{q+i}\right)\bar{x}_j
\end{eqnarray*}

On choisit maintenant $\bar{x}$ tel que $\bar{x}_1=...=\bar{x}_q=0$.
	\begin{equation} \tag{$**$} \Rightarrow c^Tx^*=-\sum_{i=1}^m d_{q+i} b_i\end{equation}

On choisit maintenant $\bar{x}$ tel que $\bar{x}_j=0$, $\forall j\in\{1,...,q\}\backslash\{i\}$, $\bar{x}_i=1$.
	\[c_i=d_i-\sum_{k=1}^m a_{ki}d_{q+k}\]
Comme $x^*$ est optimale, on aque le vecteur $d\leq 0$. 
\begin{equation}
\tag{\raisebox{-.5ex}{\shortstack{\(\ast\)\\[-.5ex]\(\ast\ast\)}}}
-\sum_{k=1}^m a_{ki} d_{q+k} = c_i-d_i \leq c_i\end{equation}

On définit maintenant $y^*$ par : 
	\[y_i^*=d_{q+i},\ i=1,...,m\]
En particulier, $y^*\geq 0$, et 
\begin{eqnarray*}
(\raisebox{-.5ex}{\shortstack{\(\ast\)\\[-.5ex]\(\ast\ast\)}})&\Leftrightarrow& -\sum_{k=1}^p a_{ki}y_k^*\leq c_i\\
							&\Leftrightarrow& -A^Ty\leq c
\end{eqnarray*}
On en déduit que $y^*$ est admissible pour le problème dual.\\
De plus, ($**$) implique :
	\[c^Tx^*=-b^Ty^*\]
\end{dem}

\Def{}{On dit qu'une base $\gamma$ est primal-réalisable si $B^{-1}b\geq 0$.\\
De même, on dit qu'elle est duale-réalisable si $d_N^T=c_N^T-c_B^TB^{-1}N\geq 0$.}
