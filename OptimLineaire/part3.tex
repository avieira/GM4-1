\part{Programmation lin\'eaire, algotihme du simplexe}
\section{Introduction}
Un problème d'optimisation linéaire est un problème d'optimisation dans lequel le coût et les contraintes sont linéaires (ou plutôt affines).\\
Il s'agit de trouver les solutions $x\in\mathbb{R}^n$ du problème :
\begin{equation}\tag{$P_L$} \label{PL}
	\left\{ \begin{array}{c c c c} \inf_{x\in\mathbb{R}^n} & \multicolumn{3}{c}{\langle c,x\rangle} \\
						\text{s. c.}     & Ax &=& b \\
								& x &\geq& 0
	\end{array} \right.
\end{equation}

où $A$ est une matrice de raille $m\times n$, $b\in\mathbb{R}^m$, $c\in\mathbb{R}^n$.\\
$x\geq 0$ signifie que toutes les composantes de $x$ sont positives.\\
Ce problème est dit sous forme standard.

\bigskip
\underline{Remarque :} On a l'impression que \ref{PL} est un cas particulier du problème (sous forme canonique) : 
\begin{equation} \label{PC}
	\left\{ \begin{array}{c c c c} \inf_{x\in\mathbb{R}^n} & \multicolumn{3}{c}{\langle c,x\rangle} \\
						\text{s. c.}     & A'x &=& b' \\
								& Ax &\geq& b
	\end{array} \right.
\end{equation}

Mais un problème sous forme canonique peut toujours se ramener à un problème sous forme standard. En effet, la contrainte $A'x=b'$ est équivalent à $A'x\geq b'$ et $-A'x\geq -b'$. Donc \ref{PC} est équivalent à :
\begin{equation} \label{PC2} \inf_{x\in\mathbb{R}^n,\ Ax\geq b} \langle c,x\rangle\end{equation}

On introduit des variables d'écart $\lambda\in\mathbb{R}^n_+$ tel que $Ax=b+\lambda$. Donc \ref{PC2} se ramène à :
\[	\left\{ \begin{array}{c c c c} \inf_{x\in\mathbb{R}^n,\ \lambda\in\mathbb{R}^n_+} & \multicolumn{3}{c}{\langle c,x\rangle} \\
						\text{s. c.}     & Ax-\lambda &=& b \\
								& b &>& 0
	\end{array} \right.\]

On décompose $x$ sous la forme :
	\[x=x^+-x^-\]
où $x^+=\max (0,x) \geq 0$ et $x^-=-\min (0,x)\geq 0$ \\
\ref{PC2} revient donc à résoudre :
\[	\left\{ \begin{array}{c c c c} \inf_{x^+\in\mathbb{R}^n, x^-\in\mathbb{R}^n, \lambda\in\mathbb{R}^n}& \multicolumn{3}{c}{\langle c,x\rangle} \\
						\text{s. c.}     & Ax-\lambda &=& b \\
								& b &>& 0
	\end{array} \right.\]
qui est bien sous forme standard (mais avec plus de variables).

\bigskip
\underline{Remarque :} On peut supposer sans perte de généralité que toutes les lignes de $A$ sont linéairement indépendantes.\\
Si ce n'est pas le cas, soit certaines constraintes sont redondantes, soit les contraintes sont incompatibles, ie $rg(A)=m\leq n$

\Def{}{L'ensemble \[X_{ad}=\{x\in\mathbb{R}^n,\ Ax=b,\ x\geq 0\}\]
est appelé l'ensemble des solutions réalisables (ou admissibles).\\
On appelle sommet (ou point extremal) de $X_{ad}$ un point $x\in X_{ad}$ tel qu'il n'existe pas $\alpha\in]0,1[$ et $y,z\in X_{ad}$, $y\neq z$ tel que $x=\alpha y+(1-\alpha)z$.}

\section{Solutions de base d'un problème sous forme standard}
On note $A_1=\begin{pmatrix} a_{1,1} \\ \vdots \\ a_{m,1} \end{pmatrix}$, ..., $A_n=\begin{pmatrix} a_{1,n} \\ \vdots \\ a_{m,n} \end{pmatrix}$. \\
Comme $rg(A)=m$, on peut toujours trouver $m$ colonnes de $A$ linéairement indépendantes.\\
On note
	\[\Gamma=\{\gamma:\{1,...,m\}\to \{1,...,n\}, \text{ strictement croissante} \}\]
On définit :
	\[A_{\gamma}=(A_{\gamma(1)} \cdots A_{\gamma(m)})\]
et :
	\[\mathcal{B}=\{\gamma\in\Gamma,\ rg(A_{\gamma})=m\}\]

Pour $\gamma\in\Gamma$, on définit $\hat{\gamma}$ comme l'unique application strictement croissante de $\{1,...,n-m\}$ dans $\{1,...,n\}$ tel que :
	\[\gamma(\{1,...,m\})\cup\gamma(\{1,...,n-m\})=\{1,...,n\}\]

\Def{}{Pour $\gamma\in\mathcal{B}$, la matrice $A_{\gamma}$ est appelée base associée à $\gamma$.\\
Les composantes $(x_{\gamma(1)},...,x_{\gamma(m)})$ sont appelées les composantes de base, et les composantes $(x_{\hat{\gamma}(1)},...,x_{\hat{\gamma}(n-m)})$ sont appelées les composantes hors base.}

Pour $\gamma\in\mathcal{B}$, on note
	\[x_{\mathcal{B}}=(x_{\gamma(1)},...,x_{\gamma(m)})\]
	\[x_{N}=(x_{\hat{\gamma(1)}},...,x_{\hat{\gamma(m)}})\]
	\[B=A_{\gamma}\]
	\[N=A_{\hat{\gamma}}\]
\[Ax=Bx_{\mathcal{B}}+Nx_N\]

Comme $rg(B)=rg(A_{\gamma})=m$, B est inversible. Donc les contraintes $Ax=b$ peuvent se réécrire :
	\[Bx_{\mathcal{B}}=b-Nx_N\]
	\[\Rightarrow B^{-1}(b-Nx_N)\]

\Def{}{Soit $\gamma\in\mathcal{B}$, on appelle solution de base du système $Ax=b$ associé à la base $\gamma$ la solution $x^*$ définie par :
	\[\begin{array}{c c c}
	x_{\mathcal{B}}^*&=&B^{-1}b\\
	x_N^*&=&0
	\end{array}\]}


\Def{}{Une solution de base réalisable est une solution de base tel que $x_{\mathcal{B}}\geq0$ ($\rightarrow x^*\in X_{ad}$).\\
Dans ce cas, $\gamma$ est appelée base réalisable. On note $\mathcal{R}$ l'ensemble des bases réallisables.\\
Enfin on dit que $x^*$ est non dégénéré si $x_{\mathcal{B}}^*>0$ (ie $B^{-1}b>0$).}

\Lem{}{Les sommets de $X_{ad}$ sont exactement les solutions de base réalisable.}

\begin{dem}
Soit $x^*$ une solution de base réalisable associée à la base $\gamma\in\mathcal{R}$.\\
Par l'absurde, on va supposer que $x^*$ n'est pas un sommet de $X_{ad}$/ 
	\[\forall \theta\in]0,1[, \exists y,z\in X_{ad}, y\neq z; x^*=\theta y+(1-\theta)z\]
$\forall i\in\{0,...,n-m\}$, on a :
	\[x_{\hat{\gamma}(i)}=0=\underbrace{\theta}_{>0} \underbrace{y_{\hat{\gamma}(i)}}_{\geq 0} + \underbrace{(1-\theta)}_{>0}\underbrace{z_{\hat{\gamma}(i)}}_{\geq 0}\]

	\[\Rightarrow y_{\hat{\gamma}(i)}=z_{\hat{\gamma}(i)}=0\]
	\[\Rightarrow y_N=z_N=0\]

Or, $y_B=B^{-1}(b-Ny_N)=B^{-1}b=z_B=x^*_B$. Donc $y=z=x^*$, ce qui est absurde.\\
Donc $x^*$ est un sommet de $X_{ad}$.

\bigskip
Réciproquement, on suppose que $x^*$ est un sommet de $X_{ad}$. On note $k$ le nombre de composantes non nulles de $x^*$. 
\[\exists \gamma_1 : \{1,...,k\}\to\{1,...,n\} \text{ strictement croissante telle que } x^*_{\gamma_1(i)}>0,\ \forall i=1,...,k\]
On veut montrer que les vecteurs $A_{\gamma_1(1)},...,A_{\gamma_1(k)}$ sont libres. Si c'est le cas, on pourra compléter cette famille de vecteurs afin d'obtenir une base.\\
($\Rightarrow$ On définit $\gamma\in\Gamma$ telle que :\begin{itemize}
\item $\gamma_1(\{1,...,k\}\subset\gamma(\{1,...,m\})$
\item $\gamma\in\mathcal{B}$
\end{itemize})

Par l'absurde, on suppose que $A_{\gamma_1(1)},...,A_{\gamma_1(k)}$ sont liés. Alors $\exists y\in\mathbb{R}^n$, $y\neq 0$, 
\[\sum_{i=1}^k y_{\gamma_1(i)}A_{\gamma_1(i)}=0\]
et \[y_{\hat{\gamma}_1(i)}=0\]
$\forall \varepsilon>0$, on a \[A(x+\varepsilon y)=Ax+\varepsilon \underbrace{Ay}_{=0}=b\]
\[A(x-\varepsilon y)=Ax-\varepsilon Ay = b\]
On a $x^*_{\gamma_1(i)}\pm \varepsilon y_{\gamma_1(i)}>0$, $\forall i=1,...,k$ pour $\varepsilon$ assez petit.\\
De plus, $x^*_{\hat{\gamma}_1(i)}\pm \varepsilon y_{\hat{\gamma}_1(i)}=x^*_{\hat{\gamma}_1(i)}=0$
	\[\Rightarrow x^*+\varepsilon y\in X_{ad},\ \forall0\leq \varepsilon \leq \varepsilon_0\]
Or, $x^*=\frac{1}{2}(x^*+\varepsilon y) + \frac{1}{2}(x^*-\varepsilon y)$. Donc $x^*$ n'est pas un sommet de $X_{ad}$. Contradiction.
\end{dem}

\Lem{}{S'il existe une solution optimale de $(P_L)$ alors il existe une solution optimale de base réalisable.}

\begin{dem}
Soit $x\in X_{ad}$ une solution optimale. On note $k$ le nombre de composantes non nulles de $x$. Soit $\gamma_1:\{1,...,k\}\to \{1,...,n\}$ strictement croissante telle que $x_{\gamma_1(i)}>0$ $\forall i=1,...,k$.\\
Si la famille $A_{\gamma_1(1)},...,A_{\gamma_1(k)}$ est libre alors $x$ est une solution optimale de base réalisable.\\
Si la famille $A_{\gamma_1(1)},...,A_{\gamma_1(k)}$ est liée, alors $\exists y\in\mathbb{R}^n$ tel que :
	\[\sum_{i=1}^k y_{\gamma_1(i)} A_{\gamma_1(i)}=0\]
	\[y_{\hat{\gamma}_1(i)}=0\ \forall i 1\leq i\leq n-k\]
$x+\varepsilon y\in X_{ad}$ pour $\varepsilon$ assez petit (cf démo précédente). \\
Comme $c$ est un point de minimum, on a :
	\[\langle c,x\rangle \leq \langle c, x\pm \varepsilon y\rangle\]
	\[\Rightarrow \pm\langle c, y\rangle \geq 0\]
	\[\langle c,y\rangle=0\]
On pose $\varepsilon_1=\sup\{\varepsilon>0;\ x\pm\varepsilon y\in X_{ad}\}$. 
	\[\exists i\in \{1,...,k\} \text{ tel que } (x+\varepsilon y)_{\gamma_1(i)}=0 \text{ ou } (x-\varepsilon y)_{\gamma_1(i)}=0\]
On pose $z=x+\varepsilon y$ ou $z=x-\varepsilon$. tel que $z_{\gamma_1(i)}=0$. z est donc une solution otpimale 
\[(\langle c,z\rangle=\langle c,x\pm\varepsilon y\rangle = \langle c,x\rangle)\]
qui a au plus $k-1$ composantes non nulles.\\
On peut refaire la preuve en remplaçant $x$ par $z$ (et en diminuant $k$) jusqu'à obtenir une famille libre.
\end{dem}
