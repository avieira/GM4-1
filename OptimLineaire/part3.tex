\part{Programmation lin\'eaire, algotihme du simplexe}
\section{Introduction}
Un problème d'optimisation linéaire est un problème d'optimisation dans lequel le coût et les contraintes sont linéaires (ou plutôt affines).\\
Il s'agit de trouver les solutions $x\in\mathbb{R}^n$ du problème :
\begin{equation}\tag{$P_L$} \label{PL}
	\left\{ \begin{array}{c c c c} \inf_{x\in\mathbb{R}^n} & \multicolumn{3}{c}{\langle c,x\rangle} \\
						\text{s. c.}     & Ax &=& b \\
								& x &\geq& 0
	\end{array} \right.
\end{equation}

où $A$ est une matrice de raille $m\times n$, $b\in\mathbb{R}^m$, $c\in\mathbb{R}^n$.\\
$x\geq 0$ signifie que toutes les composantes de $x$ sont positives.\\
Ce problème est dit sous forme standard.

\bigskip
\underline{Remarque :} On a l'impression que \ref{PL} est un cas particulier du problème (sous forme canonique) : 
\begin{equation} \label{PC}
	\left\{ \begin{array}{c c c c} \inf_{x\in\mathbb{R}^n} & \multicolumn{3}{c}{\langle c,x\rangle} \\
						\text{s. c.}     & A'x &=& b' \\
								& Ax &\geq& b
	\end{array} \right.
\end{equation}

Mais un problème sous forme canonique peut toujours se ramener à un problème sous forme standard. En effet, la contrainte $A'x=b'$ est équivalent à $A'x\geq b'$ et $-A'x\geq -b'$. Donc \ref{PC} est équivalent à :
\begin{equation} \label{PC2} \inf_{x\in\mathbb{R}^n,\ Ax\geq b} \langle c,x\rangle\end{equation}

On introduit des variables d'écart $\lambda\in\mathbb{R}^n_+$ tel que $Ax=b+\lambda$. Donc \ref{PC2} se ramène à :
\[	\left\{ \begin{array}{c c c c} \inf_{x\in\mathbb{R}^n,\ \lambda\in\mathbb{R}^n_+} & \multicolumn{3}{c}{\langle c,x\rangle} \\
						\text{s. c.}     & Ax-\lambda &=& b \\
								& b &>& 0
	\end{array} \right.\]

On décompose $x$ sous la forme :
	\[x=x^+-x^-\]
où $x^+=\max (0,x) \geq 0$ et $x^-=-\min (0,x)\geq 0$ \\
\ref{PC2} revient donc à résoudre :
\[	\left\{ \begin{array}{c c c c} \inf_{x^+\in\mathbb{R}^n, x^-\in\mathbb{R}^n, \lambda\in\mathbb{R}^n}& \multicolumn{3}{c}{\langle c,x\rangle} \\
						\text{s. c.}     & Ax-\lambda &=& b \\
								& b &>& 0
	\end{array} \right.\]
qui est bien sous forme standard (mais avec plus de variables).

\bigskip
\underline{Remarque :} On peut supposer sans perte de généralité que toutes les lignes de $A$ sont linéairement indépendantes.\\
Si ce n'est pas le cas, soit certaines constraintes sont redondantes, soit les contraintes sont incompatibles, ie $rg(A)=m\leq n$

\Def{}{L'ensemble \[X_{ad}=\{x\in\mathbb{R}^n,\ Ax=b,\ x\geq 0\}\]
est appelé l'ensemble des solutions réalisables (ou admissibles).\\
On appelle sommet (ou point extremal) de $X_{ad}$ un point $x\in X_{ad}$ tel qu'il n'existe pas $\alpha\in]0,1[$ et $y,z\in X_{ad}$, $y\neq z$ tel que $x=\alpha y+(1-\alpha)z$.}

\section{Solutions de base d'un problème sous forme standard}
On note $A_1=\begin{pmatrix} a_{1,1} \\ \vdots \\ a_{m,1} \end{pmatrix}$, ..., $A_n=\begin{pmatrix} a_{1,n} \\ \vdots \\ a_{m,n} \end{pmatrix}$. \\
Comme $rg(A)=m$, on peut toujours trouver $m$ colonnes de $A$ linéairement indépendantes.\\
On note
	\[\Gamma=\{\gamma:\{1,...,m\}\to \{1,...,n\}, \text{ strictement croissante} \}\]
On définit :
	\[A_{\gamma}=(A_{\gamma(1)} \cdots A_{\gamma(m)})\]
et :
	\[\mathcal{B}=\{\gamma\in\Gamma,\ rg(A_{\gamma})=m\}\]

Pour $\gamma\in\Gamma$, on définit $\hat{\gamma}$ comme l'unique application strictement croissante de $\{1,...,n-m\}$ dans $\{1,...,n\}$ tel que :
	\[\gamma(\{1,...,m\})\cup\gamma(\{1,...,n-m\})=\{1,...,n\}\]

\Def{}{Pour $\gamma\in\mathcal{B}$, la matrice $A_{\gamma}$ est appelée base associée à $\gamma$.\\
Les composantes $(x_{\gamma(1)},...,x_{\gamma(m)})$ sont appelées les composantes de base, et les composantes $(x_{\hat{\gamma}(1)},...,x_{\hat{\gamma}(n-m)})$ sont appelées les composantes hors base.}

Pour $\gamma\in\mathcal{B}$, on note
	\[x_{\mathcal{B}}=(x_{\gamma(1)},...,x_{\gamma(m)})\]
	\[x_{N}=(x_{\hat{\gamma(1)}},...,x_{\hat{\gamma(m)}})\]
	\[B=A_{\gamma}\]
	\[N=A_{\hat{\gamma}}\]
\[Ax=Bx_{\mathcal{B}}+Nx_N\]

Comme $rg(B)=rg(A_{\gamma})=m$, B est inversible. Donc les contraintes $Ax=b$ peuvent se réécrire :
	\[Bx_{\mathcal{B}}=b-Nx_N\]
	\[\Rightarrow B^{-1}(b-Nx_N)\]

\Def{}{Soit $\gamma\in\mathcal{B}$, on appelle solution de base du système $Ax=b$ associé à la base $\gamma$ la solution $x^*$ définie par :
	\[\begin{array}{c c c}
	x_{\mathcal{B}}^*&=&B^{-1}b\\
	x_N^$&=&0
	\end{array}\]}

\Def{}{Une solution de base réalisable est une solution de base tel que $x_{\mathcal{B}}\geq0$ ($\rightarrow x^*\in X_{ad}$).\\
Dans ce cas, $\gamma$ est appelée base réalisable. On note $\mathcal{R}$ l'ensemble des bases réallisables.\\
Enfin on dit que $x^*$ est non dégénéré si $x_{\mathcal{B}}^*>0$ (ie $B^{-1}b>0$).}

\Lem{}{Les sommets de $X_{ad}$ sont exactement les solutions de base réalisable.}

\begin{dem}
Soit $x^*$ une solution de base réalisable associée à la base $\gamma\in\mathcal{R}$.\\
Par l'absurde, on va supposer que $x^*$ n'est pas un sommet de $X_{ad}$/ 
	\[\forall \theta\in]0,1[, \exists y,z\in X_{ad}, y\neq z; x^*=\theta y+(1-\theta)z\]
$\forall i\in\{0,...,n-m\}$, on a :
	\[x_{\hat{\gamma}(i)}=0=\underbrace{\theta}_{>0} \underbrace{y_{\hat{\gamma}(i)}}_{\geq 0} + \underbrace{(1-\theta)}_{>0}\underbrace{z_{\hat{\gamma}(i)}}_{\geq 0}\]

	\[\Rightarrow y_{\hat{\gamma}(i)}=z_{\hat{\gamma}(i)}=0\]
	\[\Rightarrow y_N=z_N=0\]

Or, $Y_B=B^{-1}(b-Ny_N)=B^{-1}b=z_B=x^*_B$. Donc $y=z=x^*$, ce qui est absurde.\\
Donc $x^*$ est un sommet de $X_{ad}$.

\bigskip
\end{dem}
