\part{Programmation lin\'eaire, algotihme du simplexe}
\section{Introduction}
Un problème d'optimisation linéaire est un problème d'optimisation dans lequel le coût et les contraintes sont linéaires (ou plutôt affines).\\
Il s'agit de trouver les solutions $x\in\mathbb{R}^n$ du problème :
\begin{equation}\tag{$P_L$} \label{PL}
	\left\{ \begin{array}{c c c c} \inf_{x\in\mathbb{R}^n} & \multicolumn{3}{c}{\langle c,x\rangle} \\
						\text{s. c.}     & Ax &=& b \\
								& x &\geq& 0
	\end{array} \right.
\end{equation}

où $A$ est une matrice de raille $m\times n$, $b\in\mathbb{R}^m$, $c\in\mathbb{R}^n$.\\
$x\geq 0$ signifie que toutes les composantes de $x$ sont positives.\\
Ce problème est dit sous forme standard.

\bigskip
\underline{Remarque :} On a l'impression que \ref{PL} est un cas particulier du problème (sous forme canonique) : 
\begin{equation} \label{PC}
	\left\{ \begin{array}{c c c c} \inf_{x\in\mathbb{R}^n} & \multicolumn{3}{c}{\langle c,x\rangle} \\
						\text{s. c.}     & A'x &=& b' \\
								& Ax &\geq& b
	\end{array} \right.
\end{equation}

Mais un problème sous forme canonique peut toujours se ramener à un problème sous forme standard. En effet, la contrainte $A'x=b'$ est équivalent à $A'x\geq b'$ et $-A'x\geq -b'$. Donc \ref{PC} est équivalent à :
\begin{equation} \label{PC2} \inf_{x\in\mathbb{R}^n,\ Ax\geq b} \langle c,x\rangle\end{equation}

On introduit des variables d'écart $\lambda\in\mathbb{R}^n_+$ tel que $Ax=b+\lambda$. Donc \ref{PC2} se ramène à :
\[	\left\{ \begin{array}{c c c c} \inf_{x\in\mathbb{R}^n,\ \lambda\in\mathbb{R}^n_+} & \multicolumn{3}{c}{\langle c,x\rangle} \\
						\text{s. c.}     & Ax-\lambda &=& b \\
								& b &>& 0
	\end{array} \right.\]

On décompose $x$ sous la forme :
	\[x=x^+-x^-\]
où $x^+=\max (0,x) \geq 0$ et $x^-=-\min (0,x)\geq 0$ \\
\ref{PC2} revient donc à résoudre :
\[	\left\{ \begin{array}{c c c c} \inf_{x^+\in\mathbb{R}^n, x^-\in\mathbb{R}^n, \lambda\in\mathbb{R}^n}& \multicolumn{3}{c}{\langle c,x\rangle} \\
						\text{s. c.}     & Ax-\lambda &=& b \\
								& b &>& 0
	\end{array} \right.\]
qui est bien sous forme standard (mais avec plus de variables).

\bigskip
\underline{Remarque :} On peut supposer sans perte de généralité que toutes les lignes de $A$ sont linéairement indépendantes.\\
Si ce n'est pas le cas, soit certaines constraintes sont redondantes, soit les contraintes sont incompatibles, ie $rg(A)=m\leq n$

\Def{}{L'ensemble \[X_{ad}=\{x\in\mathbb{R}^n,\ Ax=b,\ x\geq 0\}\]
est appelé l'ensemble des solutions réalisables (ou admissibles).\\
On appelle sommet (ou point extremal) de $X_{ad}$ un point $x\in X_{ad}$ tel qu'il n'existe pas $\alpha\in]0,1[$ et $y,z\in X_{ad}$, $y\neq z$ tel que $x=\alpha y+(1-\alpha)z$.}

\section{Solutions de base d'un problème sous forme standard}
On note $A_1=\begin{pmatrix} a_{1,1} \\ \vdots \\ a_{m,1} \end{pmatrix}$, ..., $A_n=\begin{pmatrix} a_{1,n} \\ \vdots \\ a_{m,n} \end{pmatrix}$. \\
Comme $rg(A)=m$, on peut toujours trouver $m$ colonnes de $A$ linéairement indépendantes.\\
On note
	\[\Gamma=\{\gamma:\{1,...,m\}\to \{1,...,n\}, \text{ strictement croissante} \}\]
On définit :
	\[A_{\gamma}=(A_{\gamma(1)} \cdots A_{\gamma(m)})\]
et :
	\[\mathcal{B}=\{\gamma\in\Gamma,\ rg(A_{\gamma})=m\}\]

Pour $\gamma\in\Gamma$, on définit $\hat{\gamma}$ comme l'unique application strictement croissante de $\{1,...,n-m\}$ dans $\{1,...,n\}$ tel que :
	\[\gamma(\{1,...,m\})\cup\gamma(\{1,...,n-m\})=\{1,...,n\}\]

\Def{}{Pour $\gamma\in\mathcal{B}$, la matrice $A_{\gamma}$ est appelée base associée à $\gamma$.\\
Les composantes $(x_{\gamma(1)},...,x_{\gamma(m)})$ sont appelées les composantes de base, et les composantes $(x_{\hat{\gamma}(1)},...,x_{\hat{\gamma}(n-m)})$ sont appelées les composantes hors base.}

Pour $\gamma\in\mathcal{B}$, on note
	\[x_{\mathcal{B}}=(x_{\gamma(1)},...,x_{\gamma(m)})\]
	\[x_{N}=(x_{\hat{\gamma(1)}},...,x_{\hat{\gamma(m)}})\]
	\[B=A_{\gamma}\]
	\[N=A_{\hat{\gamma}}\]
\[Ax=Bx_{\mathcal{B}}+Nx_N\]

Comme $rg(B)=rg(A_{\gamma})=m$, B est inversible. Donc les contraintes $Ax=b$ peuvent se réécrire :
	\[Bx_{\mathcal{B}}=b-Nx_N\]
	\[\Rightarrow B^{-1}(b-Nx_N)\]

\Def{}{Soit $\gamma\in\mathcal{B}$, on appelle solution de base du système $Ax=b$ associé à la base $\gamma$ la solution $x^*$ définie par :
	\[\begin{array}{c c c}
	x_{\mathcal{B}}^*&=&B^{-1}b\\
	x_N^*&=&0
	\end{array}\]}


\Def{}{Une solution de base réalisable est une solution de base tel que $x_{\mathcal{B}}\geq0$ ($\rightarrow x^*\in X_{ad}$).\\
Dans ce cas, $\gamma$ est appelée base réalisable. On note $\mathcal{R}$ l'ensemble des bases réallisables.\\
Enfin on dit que $x^*$ est non dégénéré si $x_{\mathcal{B}}^*>0$ (ie $B^{-1}b>0$).}

\Lem{}{Les sommets de $X_{ad}$ sont exactement les solutions de base réalisable.}

\begin{dem}
Soit $x^*$ une solution de base réalisable associée à la base $\gamma\in\mathcal{R}$.\\
Par l'absurde, on va supposer que $x^*$ n'est pas un sommet de $X_{ad}$/ 
	\[\forall \theta\in]0,1[, \exists y,z\in X_{ad}, y\neq z; x^*=\theta y+(1-\theta)z\]
$\forall i\in\{0,...,n-m\}$, on a :
	\[x_{\hat{\gamma}(i)}=0=\underbrace{\theta}_{>0} \underbrace{y_{\hat{\gamma}(i)}}_{\geq 0} + \underbrace{(1-\theta)}_{>0}\underbrace{z_{\hat{\gamma}(i)}}_{\geq 0}\]

	\[\Rightarrow y_{\hat{\gamma}(i)}=z_{\hat{\gamma}(i)}=0\]
	\[\Rightarrow y_N=z_N=0\]

Or, $y_B=B^{-1}(b-Ny_N)=B^{-1}b=z_B=x^*_B$. Donc $y=z=x^*$, ce qui est absurde.\\
Donc $x^*$ est un sommet de $X_{ad}$.

\bigskip
Réciproquement, on suppose que $x^*$ est un sommet de $X_{ad}$. On note $k$ le nombre de composantes non nulles de $x^*$. 
\[\exists \gamma_1 : \{1,...,k\}\to\{1,...,n\} \text{ strictement croissante telle que } x^*_{\gamma_1(i)}>0,\ \forall i=1,...,k\]
On veut montrer que les vecteurs $A_{\gamma_1(1)},...,A_{\gamma_1(k)}$ sont libres. Si c'est le cas, on pourra compléter cette famille de vecteurs afin d'obtenir une base.\\
($\Rightarrow$ On définit $\gamma\in\Gamma$ telle que :\begin{itemize}
\item $\gamma_1(\{1,...,k\}\subset\gamma(\{1,...,m\})$
\item $\gamma\in\mathcal{B}$
\end{itemize})

Par l'absurde, on suppose que $A_{\gamma_1(1)},...,A_{\gamma_1(k)}$ sont liés. Alors $\exists y\in\mathbb{R}^n$, $y\neq 0$, 
\[\sum_{i=1}^k y_{\gamma_1(i)}A_{\gamma_1(i)}=0\]
et \[y_{\hat{\gamma}_1(i)}=0\]
$\forall \varepsilon>0$, on a \[A(x+\varepsilon y)=Ax+\varepsilon \underbrace{Ay}_{=0}=b\]
\[A(x-\varepsilon y)=Ax-\varepsilon Ay = b\]
On a $x^*_{\gamma_1(i)}\pm \varepsilon y_{\gamma_1(i)}>0$, $\forall i=1,...,k$ pour $\varepsilon$ assez petit.\\
De plus, $x^*_{\hat{\gamma}_1(i)}\pm \varepsilon y_{\hat{\gamma}_1(i)}=x^*_{\hat{\gamma}_1(i)}=0$
	\[\Rightarrow x^*+\varepsilon y\in X_{ad},\ \forall0\leq \varepsilon \leq \varepsilon_0\]
Or, $x^*=\frac{1}{2}(x^*+\varepsilon y) + \frac{1}{2}(x^*-\varepsilon y)$. Donc $x^*$ n'est pas un sommet de $X_{ad}$. Contradiction.
\end{dem}

\Lem{}{S'il existe une solution optimale de $(P_L)$ alors il existe une solution optimale de base réalisable.}

\begin{dem}
Soit $x\in X_{ad}$ une solution optimale. On note $k$ le nombre de composantes non nulles de $x$. Soit $\gamma_1:\{1,...,k\}\to \{1,...,n\}$ strictement croissante telle que $x_{\gamma_1(i)}>0$ $\forall i=1,...,k$.\\
Si la famille $A_{\gamma_1(1)},...,A_{\gamma_1(k)}$ est libre alors $x$ est une solution optimale de base réalisable.\\
Si la famille $A_{\gamma_1(1)},...,A_{\gamma_1(k)}$ est liée, alors $\exists y\in\mathbb{R}^n$ tel que :
	\[\sum_{i=1}^k y_{\gamma_1(i)} A_{\gamma_1(i)}=0\]
	\[y_{\hat{\gamma}_1(i)}=0\ \forall i 1\leq i\leq n-k\]
$x+\varepsilon y\in X_{ad}$ pour $\varepsilon$ assez petit (cf démo précédente). \\
Comme $c$ est un point de minimum, on a :
	\[\langle c,x\rangle \leq \langle c, x\pm \varepsilon y\rangle\]
	\[\Rightarrow \pm\langle c, y\rangle \geq 0\]
	\[\langle c,y\rangle=0\]
On pose $\varepsilon_1=\sup\{\varepsilon>0;\ x\pm\varepsilon y\in X_{ad}\}$. 
	\[\exists i\in \{1,...,k\} \text{ tel que } (x+\varepsilon y)_{\gamma_1(i)}=0 \text{ ou } (x-\varepsilon y)_{\gamma_1(i)}=0\]
On pose $z=x+\varepsilon y$ ou $z=x-\varepsilon$. tel que $z_{\gamma_1(i)}=0$. z est donc une solution otpimale 
\[(\langle c,z\rangle=\langle c,x\pm\varepsilon y\rangle = \langle c,x\rangle)\]
qui a au plus $k-1$ composantes non nulles.\\
On peut refaire la preuve en remplaçant $x$ par $z$ (et en diminuant $k$) jusqu'à obtenir une famille libre.
\end{dem}

\underline{Remarque :} D'après les deux propositions précédentes, en prenant $c=0$ (toutes les solutions admissibles sont optimales), si $X_{ad}=\emptyset$, alors $X_{ad}$ possède au moins 1 sommet (ceci n'est pas le cas pour un polyèdre quelconque).

\section{Algorithme du simplexe}
\underline{Idée :} Parcourir les sommets du polyèdre en diminuant le coût en passant d'un sommet au suvant.

\Lem{}{Soit $\gamma\in\mathbb{R}$ et $x^*$ la solution de base réalisable associée.\\
Pour $x\in X_{ad}$ réalisable, on a :
	\[X_B=X^*_B-B^{-1}Nx_N\]
	\[c^Tx=X^Tx^*-d^Tx\]
où $d^T_N=cN^T-C_B^TB^{-1}N$ et $d^T_B=0$.}

\begin{dem}
\begin{eqnarray*}
	Ax&=&Bx_Nx_N=b\\
\rightarrow x_B&=&B^{-1}b-B^{-1}Nx_N
\end{eqnarray*}
Or, $x^*_B=B^{-1}b$, donc \[x_B=x^*_B-B^{-1}Nx_N\]

\begin{eqnarray*}
	c^Tx&=&c_B^Tx_B+c^T_Nx_n\\
	&=&c^T_B(x^*_B-B^{-1}Nx_N)+x^T_N x_N\\
	&=&c^T_Bx_B^*+(c^T_N-c^T_BB^{-1}N)x_N\\
	&=&c^Tx^*+d^Tx
\end{eqnarray*}
\end{dem}

\Def{}{On dit que le vecteur $d$ est le veteur des prix marginaux associé à la base $\gamma$.}

\Propo{}{Soit $\gamma$ une base réalisable et $x^*$ la solution de base associée.\\
Si toutes les composantes de $d$ sont positives, alors $s^*$ est une solution otpimale.\\
Si de plus, $x^*$ est non dégénérée ($B^{-1}b>0$) et est une solution optimale, alors les composantes de $d$ sont positives.}

\begin{dem}
Soit $x$ une solution réalisable. alors 
\[c^Tx=x^Tx^*+\underbrace{d^T}_{\geq 0}\underbrace{x}_{\geq 0}\geq c^Tx^*\]
$x^*$ est donc une solution optimale.

\bigskip
Réciproquement, on suppose que $x^*$ est une solution optimale non dégénérée.\\
Raisonnons par l'absurde : supposons que la ième composante de $d$ est strictement négative.\\
Pour $\varepsilon>0$, on définit :
\begin{eqnarray*}
	x_N(\varepsilon)&=&\varepsilon a_i\\
	x_B(\varepsilon)&=&B^{-1}b-B^{-1}Nx_N(\varpesilon)
\end{eqnarray*}

On a :
\begin{eqnarray*}
	Ax(\varepsilon)&=&Bx_B(\varepsilon)+Nx_N(\varepsilon)\\
			&=&b
\end{eqnarray*}
Comme $\varepsilon>0$ on a $x_N(\varepsilon)>0$.\\
Comme $B^{-1}b>0$, pour $\varepsilon$ assez petit, on a $x_B(\varepsilon)\geq 0$.\\
\[\Rightarrow x(\varepsilon)>0\]
$\Rightarrow$ $x(\varepsilon)$ est une solution réalisable.

\bigskip
On a également :
\begin{eqnarray*}
	c^Tx(\varepsilon)&=&c^Tx^*+d^Tx(\varepsilon)\\
		&=&c^Tx^*+d^T_Nx_N(\varepsilon)\\
		&=&c^Tx^*+\varepsilon \underbrace{\d^T_{\hat{\gamma}(i)}}_{<0}\\
		&<&c^Tx^*
\end{eqnarray*}
$\Rightarrow$ $x^*$ n'est pas otimal. Absurde.
\end{dem}

\subsection{Privot à partir d'une base réalisable : critère de Dantzig}
Étant donné un choix de base $\gamma\in B$, tel que $x^*$ soit une solution de base réalisable mais pas otimale, le but est de trouver une nouvelle base $\delta\in B$ tel que la solution de base $y^*$ associée à $S$ soit réalisable et tel que 
\[c^Ty^*\leq c^Tx^*\]
Cette méthode opère au moyen d'un pivot dans le sens où $\gamma(\{1,...,m\})$ et $\delta(\{1,...,m\})$ ne diffèrent que par 1 seul élément.\\
En pratique, on note 
	\[E_{\gamma}=j\in\{1,...,n-m\};\ d_{\hat{\gamma}(j)}<0\}\]
Si la solution de base $x^*$ associée à $\gamma$ n'est pas optimale, alors $E_{\gamma}\neq \emptyset$.\\
Soit $j\in E_{\gamma}$ (plusieurs choix possibles, on verra ça plus tard). On définit l'ensemble 
	\[S_{\gamma,j^*}=\left\{i\in\{1,...,m\};\ (B^{-1}N)_{i,j^*}>0 \right\}\]

\Propo{}{Si $S_{\gamma,j^*}=\emptyset$, alors le problème $(P_L)$ n'admet pas de solution car la fonction coût n'est pas bornée inférieurement sur $X_{ad}$.}

\begin{dem}
	Soit $t>0$. On considère le vecteur $x$ définit par :
\begin{eqnarray*}
	x_{\hat{\gamma}(j)}&=&0 \text{ si } j\in\{1,...,n-m\}\textbackslash \{j^*\}\\
	x_{\hat{\gamma}(j^*)}&=&t\\
	x_{\gamma(i)}&=&B^{-1}b-t(B^{-1}N)_{ij^*} \text{ si } i\in\{1,...,m\}
\end{eqnarray*}

On a $Ax=b$ par construction.\\
De plus, $x_N\geq 0$ et $x_B\geq 0$ car $(B^{-1}N)_{ij^*}\leq 0$ (car $S_{\gamma,j^*}=\emptyset$).\\
$\Rightarrow$ $x$ est une solution admissible.

\bigskip
De plus :
\begin{eqnarray*}
	c^Tx&=&x^Tx^*+d^Tx\\
		&=&c^Tx^*+d_N^Tx_N\\
		&=&c^Tx^*+\underbrace{d_{\hat{\gamma}(j^*)}}_{<0}t
\end{eqnarray*}

Quant $t\to +\infty$, on a $c^Tx\to -\infty$, avec $x\in X^{ad}$. Ma fonction coût n'est donc pas bornée inférieurement.
\end{dem}

On suppose à partir de maintenant que la fonction coût est bornée inférieurement.
	\[\Rightarrow S_{\gamma,j^*}\neq \emptyset\]
On choisit $i^*\inS_{\gamma,j^*}$.

\Lem{}{Soit $\delta\in\Gamma$ l'unique application telle que 
	\[\delta(\{1,...,m\})=\hat{\gamma}(\{1,...,m\}\textbackslash\gamma(i^*)\cup\hat{\gamma}(j^*)\]
Alors $\delta\in B$.}

\begin{dem}
Montrons que :
	\[A_{\hat{\gamma}(j^*)}=\sum_{i=1}^m (B^{-1}N)_{ij^*}A_{\gamma(i)}\]
\begin{eqnarray*}
	\sum_{i=1}^m (B^{-1}N)_{ij^*}A_{\gamma(i)}&=&\sum_{i=1}^m \sum_{j=1}^m B^{-1}_{ij}N_{jj^*}A_{\gamma(i)}\\
			&=&\sum_{j=1}^* N_{jj^*} \sum_{i=1}^m B^{-1}_{ij}\begin{pmatrix} B_{1i}\\ \vdots \\ B_{mi} \end{pmatrix} \\
			&=&\sum_{j=1}^m N_{jj^*} \begin{pmatrix} \sum_{i=1}^m B_{1i}B^{-1}_{ij} \\ \vdots \\ \sum_{i=1}^m B_{mi}B^{-1}_{ij} \end{pmatrix} \\
			&=&\sum_{i=1}^m N_{jj^*} \begin{pmatrix} (BB^{-1})_{1j} \\ \vdots \\ (BB^{-1})_{mj} \end{pmatrix}\\
			&=&\sum_{j=1}^m N_{jj^*}e_j \\
			&=&A_{\hat{\gamma}(j^*)}
\end{eqnarray*}
Comme $\{A_{\gamma(i)}\}_{i=1}^m$ forme une base et que $(B^{-1}N)_{i^*j^*}>0$, on ne peut pas exprimer $A_{\hat{gamma(j^*)}}$ comme une combinaison linéaire de $A_{\gamma(i)}$ pour $i=1,...,n$ avec $i\neq i^*$. \\
Donc $((A_{\gamma(i)})_{i=1,...,m,\ i\neq i^*},A_{\hat{\gamma}(j^*)})$ forme une base.
	\[\Rightarrow \delta \in B\]
\end{dem}
